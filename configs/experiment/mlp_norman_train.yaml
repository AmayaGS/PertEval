# @package _global_

# run with `python src/train.py experiment=mlp_norman_train`

model_type: "mlp" # pick from mlp or lr

defaults:
  - override /model: mlp
  - override /logger: wandb

total_genes: 2061 # or embedding dim for scFM embeddings

data:
  data_name: "norman"
  split: 0.00
  replicate: 0
  batch_size: 32
  fm: "raw_expression"

trainer:
  max_epochs: 30
  accelerator: gpu
  devices: 2

callbacks:
  learning_rate_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: 'epoch'

logger:
  wandb:
    tags: ["${model_type}", "norman", "split_${data.split}", "replicate_${data.replicate}", "hpo"]
    group: "${model_type}_norman_${data.split}"

model:
  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 3e-5
    weight_decay: 0.0

  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: 'min'
    factor: 0.1
    patience: 15
    min_lr: 3e-9

#hydra:
#  sweeper:
#    params:
#      +data.replicate: range(${repeats})

#  scheduler:
#    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#    _partial_: true
#    T_max: ${eval:'${trainer.max_epochs}//2'}
